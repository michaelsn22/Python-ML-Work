# -*- coding: utf-8 -*-
"""CNN on cifar-10 Dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s1_Yhc7mqcw8lbYThxvg0R1EKgxugilF
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
import numpy as np
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt
import itertools
import keras
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization
from sklearn.metrics import confusion_matrix, classification_report
#from tensorflow.keras.utils import to_categorical

# %matplotlib inline

batch_size = 64
num_classes = 10 #since we know there are 10 from dataset.
epochs = 50
data_augmentation = False #could be enabled to help train the model. this is similar to what prof referred to as "salting"

#setup training set and test set vars
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

#normalize data
x_train = x_train.astype('float32') #gets values from 0 to 1
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255

#1 hot encoding
#y_train = keras.utils.to_categorical(y_train, num_classes)
#wow this took a long time to solve....
#needed to use: from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)

model = Sequential() #groups a stack of layers into the keras model.
#begin to add multiply convolutional layers, along with their activation function and various sizes
model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])) #column
model.add(Activation('relu'))
model.add(BatchNormalization())
#2D convolution layer (e.g. spatial convolution over images).
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
#2D convolution layer (e.g. spatial convolution over images).
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())
#2D convolution layer (e.g. spatial convolution over images).
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
#using softmax since we have 10 outputs.
model.add(Dense(num_classes))
model.add(Activation('softmax'))
model.summary()

#using RMSprop for learning rate adjustment Going to use it instead of ADAM just to try something different.
#"root mean squared propagation."
opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-7)
#rms = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-7)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True)

import matplotlib
import matplotlib.pyplot as plt
fig,axs = plt.subplots(1,2,figsize=(15, 5))
axs[0].plot(history.history['accuracy'])
axs[0].plot(history.history['val_accuracy'])
axs[0].set_title('Model Accuracy')
axs[0].set_ylabel('Accuracy')
axs[0].set_xlabel('Epoch')
axs[0].legend(['train', 'validate'], loc='upper left')

axs[1].plot(history.history['loss'])
axs[1].plot(history.history['val_loss'])
axs[1].set_title('Model loss')
axs[1].set_ylabel('Loss')
axs[1].set_xlabel('Epoch')
axs[1].legend(['train', 'validate'], loc='upper left')
plt.show()

scores = model.evaluate(x_test, y_test, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])